---
layout: post
title: "Ensemble Learning"
date: 2019-11-05 08:44:38 -0400
category: machine-learning
author: eun
short-description: 다양한 모델을 결합한 앙상블 학습
toc: true
---


#### 앙상블 학습
**앙상블 학습**(`Ensemble Learning`): 여러 개의 분류기를 생성하고, 그 예측을 결합함으로써 보다 정확한 예측을 도출하는 기법

예를 들어 10명의 전문가로부터 예측을 얻을 수 있다고 가정한다. 앙상블 방법은 전문가 10명의 예측을 묶어 전문가 1명보다 더 정확하고 안정된 예측을 만들 수 있다. <u>앙상블은 하나의 분류기 보다 더 좋은 일반화 성능을 낼 수 있다.</u>

아상블의 대표적인 학습법에는 두가지가 있다. 배깅(`Bagging`)과 부스팅(`Boosting`)이다. 

#### 배깅(Bagging)
**Bagging**: 원본 훈련 데이터셋에서 부트스트랩 샘플(중복을 허용한 랜덤 샘플)을 뽑아서 모델을 학습시킨다. 

![Image Alt 텍스트](/assets/images/ml03_01.png){: width="50%" height="50%"}     
우선, 훈련 데이터셋으로부터 부트스트랩을 한다. 각각 부트스트랩 샘플 사용하여 모델을 학습시키고 학습된 모델의 결과를 집계하여 최종 결과 값을 구한다. Categorical Data는 다수결 투표로, ntinuous Data는 평균으로 집계한다. 

배깅은 간단하지만 유용한 방법이다. 이런 배깅 방법을 활용한 랜덤 포레스트이다. 

실전에서 고차원의 데이터셋을 사용하는 복잡한 분류 문제라면 단일 경정 트리는 쉽게 과대적합 될 수 있다. 이런 경우 **배깅 알고리즘**의 강력함이 발휘 될 수 있다. 배깅 알고리즘은 모델의 분산을 감소하는 효과적인 방법이지만 모델의 편향을 낮추는데 효과적이지 않다. 즉, 모델의 너무 단순해서 데이터에 있는 경향을 잘 잡아내지 못한다. 이런 이유때문에 **배깅을 수행할 때 편향이 낮은 모델을 분류기로 사용하여 앙상블을 만든다.**

#### 부스팅
**부스팅**(`Boosting`): 가중치를 활용하여 약 분류기를 강 분류기로 만드는 방법

그 중 가장 유명한 **에이다부스트**(`AdaBoost`)을 자세히 보자.
원본 부


![Image Alt 텍스트](/assets/images/ml03_02.png){: width="50%" height="50%"}   