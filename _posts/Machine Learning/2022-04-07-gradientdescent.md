---
layout: post
title: "Gradient Descent"
date: 2022-04-06 08:44:38 -0400
category: machine-learning
author: eun
short-description: 간단하지만 강력한 최적화 알고리즘, 경사하강법
toc: true
use_math: true
---

#### intro
---
경사하강법을 알기전에 손실함수와 최적화 개념을 먼저 알아야한다. 
- **목적함수**: 최소화 최대화하고 싶어하는 함수
    + 최소화시킨 함수: 비용함수, 손실함수로 부른다.
- **손실함수(`Loss Function)`**: 정답값과 예측값의 오차를 계산하는 함수
    + 파라미터 측정에 사용
    + 알고리즘이 잘 학습되고 있는지 표현하는 지표
    + 손실함수의 값이 낮을 수록 학습이 잘 된 것
+ **비용함수(`Cost Function`)**: 최적화 문제에 사용
- **최적화**: 손실 함수가 최소화되게 하는 모델 파라미터를 조정하는 것

EX) A가 부산 여행을 가려고 하는 경우    
A는 최소한의 비용으로 여행을 가길 원한다. 경비를 아끼기 위해 택시 대신 버스, 비행기 대신 기차, 호텔 대신 에어비앤비 등 적은 비용이 드는 방향으로 여행 계획을 세울 것이다. 

즉, 여기서 A의 목적 함수는 여행 경비, 여행 경비를 최소화하는 것이 목적이다. 이 상황에서 최적화는 목적함수의 최소화 문제라고 해석할 수 있다.

<br>

####  Gradient Descent
---
- **경사하강법(`Gradient Descent`)**
- **모델이 데이터를 잘 표현할 수 있도록 기울기(변화율)를 사용하여 모델을 조금씩 조정하는 최적화 알고리즘**
- 함수 기울기(경사)의 반대 방향으로 계속 이동시켜 극값에 이를 때까지 반복하는 것
- 비용 함수(`cost function`)을 최소화하는 가중치를 찾을 수 있음
- 1차 근삿값 발견용 최적화 알고리즘
- 학습을 통해 모델의 최적 파라미터를 찾는 것이 목표

예측값을 바탕으로 기울기(변화율)을 업데이트 한다.
- 예측값: 모델을 통해 예측한 값(ex. y=7*x+4라는 모델의 x에 7을 넣으면 53된다.)
- 변화율: 모델의 가중치와 절편을 **효율적으로** 업데이트 시키기 위한 값

<p align='center'><img src="/assets/images/gradient_02.png"  width="50%" height="50%"></p>
변화율을 이용하면 효율적인 방법으로 가중치를 업데이트 할 수 있다. 0보다 크거나 작은 경우 모두 가중치에 변화율을 더하기(+)만 하면 된다. 

```python
w_new = w + w_rate
```
- 가중치 변화율과 절편 변화율 공식
<p align='center'><img src="/assets/images/gradient_03.png"  width="60%" height="60%"></p>


<br>

####  Gradient Descent 과정
--- 
- **STEP 01: 특정 파라미터 값으로 시작**
    + 초기 가중치에 대한 시작 점을 선택
    + w1을 0으로 설정하거나 임의의 값을 선택
- **STEP 02: Cost Function 계산**
    + 비용 함수: 모델을 구성하는 가중치 w의 함수, 
    + 시작점에서 곡선의 기울기 계산
- **STEP 03: 파라미터 값 업데이트**
    + 파라미터: 학습을 통해 최적화하는 변수
    + $\boldsymbol{w} := \boldsymbol{w} + \mathit\Delta \boldsymbol{w}$
- **STEP 04: 반복 학습**  
    <p><img src="/assets/images/gradient_01.png"  width="50%" height="50%"></p>
    <p style="font-size: 0.5em">경사하강법 알고리즘(출처: 머신러닝 교과서)</p>



