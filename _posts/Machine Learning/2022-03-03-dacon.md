---
layout: post
title: "[DACON] 집값 예측 경진대회"
date: 2022-03-01 08:44:38 -0400
category: machine-learning
author: eun
short-description: 데이콘 집값 예측 경진대회 코드 정리
toc: true
---

####  intro
---

[https://dacon.io/competitions/official/235869/overview/description](https://dacon.io/competitions/official/235869/overview/description)

1. 배경     
train 데이터 및 test 데이터 총합 2,700개 집값 데이터가 주어진다. 이 데이터를 이용해 주어지지 않은 집값을 예측해 보자. 
2. 목적     
housing 데이터를 통한 집값 예측 
3. 평가     
- 평가 산식: NMAE

    ```python
    import numpy as np

    def NMAE(true, pred):
        mae = np.mean(np.abs(true-pred))
        score = mae / np.mean(np.abs(true))
        return score
    ```
4. 데이터
- id : 데이터 고유 id
- OverallQual : 전반적 재료와 마감 품질
- YearBuilt : 완공 연도
- YearRemodAdd : 리모델링 연도
- ExterQual : 외관 재료 품질
- BsmtQual : 지하실 높이
- TotalBsmtSF : 지하실 면적 
- 1stFlrSF : 1층 면적 
- GrLivArea : 지상층 생활 면적
- FullBath : 지상층 화장실 개수 
- KitchenQual : 부억 품질 
- GarageYrBlt : 차고 완공 연도
- GarageCars: 차고 자리 개수
- GarageArea: 차고 면적 
- target : 집값(달러 단위)

#### DataSet
---

```python
train = pd.read_csv('/content/drive/MyDrive/dacon/집값예측경진대회/train.csv', sep=',', encoding='cp949')
test = pd.read_csv('/content/drive/MyDrive/dacon/집값예측경진대회/test.csv', sep=',', encoding='cp949')
```


<br>

#### EDA
---
- 결측치 없음
- 품질 변수 4개, 년도 변수 3개, 면적 변수 6개로 구성
- 품질 변수: 카테고리 값
- 이상치가 많음
- 범주형 특정 값에 치우쳐 있음
- 예측 변수 target이 정규성을 띄지 않아 로그 변환 필요


위 EDA 결과에 관련된 코드만 첨부하였습니다. 전체 코드는 깃헙에서 확인 가능합니다.

```python
print(train.isna().sum())
print(sns.heatmap(train.isna(),cbar=False))
```
![Image Alt 텍스트](/assets/images/dacon1_1.PNG){: width="40%" height="40%"}

결측치가 존재하지 않는다

```python
numeric_feature = train.columns[(train.dtypes==int) | (train.dtypes== float)]
categorical_feature = train.columns[train.dtypes=='O']

print("수치형 데이터는 다음과 같습니다. \n", list(numeric_feature))
print("카테고리형 데이터는 다음과 같습니다. \n", list(categorical_feature))
```
![Image Alt 텍스트](/assets/images/dacon1_2.PNG){: width="100%" height="100%"}

품질 변수 3개는 카테고리형 데이터로, 라벨 인코딩이 필요하다

```python
import matplotlib.pyplot as plt
% matplotlib inline
plt.style.use("ggplot")

feature = numeric_feature

# Boxplot
plt.figure(figsize=(20,15))
plt.suptitle("Boxplots", fontsize=40)

for i in range(len(feature)):
    plt.subplot(4,3,i+1)
    plt.title(feature[i])
    plt.boxplot(train[feature[i]])
plt.show()

```
![Image Alt 텍스트](/assets/images/dacon1_3.PNG){: width="70%" height="70%"}

특정 변수들은 이상치가 존재하는 것을 확인할 수 있다.

``` python
# 히스토그램
feature = categorical_feature

plt.figure(figsize=(20,10))
plt.suptitle("Bar Plot", fontsize=40)

for i in range(len(feature)):
    plt.subplot(1,3,i+1)
    plt.title(feature[i], fontsize=20)
    temp = train[feature[i]].value_counts()
    plt.bar(temp.keys(), temp.values, width=0.5, color='b', alpha=0.5)
    plt.xticks(temp.keys(), fontsize=12)
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()
```

![Image Alt 텍스트](/assets/images/dacon1_4.PNG){: width="70%" height="70%"}

품질변수는 대부분 TA, GD에 분포하고 있음을 확인할 수 있다

``` python
#바이올린 플롯(범주형 변수 분포)
feature = categorical_feature

plt.figure(figsize=(20,6))
plt.suptitle("Violin Plot", fontsize=40)

# id는 제외하고 시각화
for i in range(len(feature)):
    plt.subplot(1,3,i+1)
    plt.xlabel(feature[i])
    plt.ylabel("target")
    sns.violinplot(x= train[feature[i]], y= train["target"])
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()
```

![Image Alt 텍스트](/assets/images/dacon1_5.PNG){: width="70%" height="70%"}

품질변수가 EX인 데이터들이 상대적으로 높은 집값에 분포되어 있다.

```python
#바이올린 플롯(범주형 변수 분포)
feature = categorical_feature

plt.figure(figsize=(20,6))
plt.suptitle("Violin Plot", fontsize=40)

# id는 제외하고 시각화
for i in range(len(feature)):
    plt.subplot(1,3,i+1)
    plt.xlabel(feature[i])
    plt.ylabel('overall qual')
    sns.violinplot(x= train[feature[i]], y= train['Overall Qual'])
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()
```

![Image Alt 텍스트](/assets/images/dacon1_6.PNG){: width="70%" height="70%"}

품질변수가 EX인 데이터들이 Overall Quall 역시 높다.


```python
from scipy import stats
from scipy.stats import norm

sns.distplot(train['Gr Liv Area'],fit = norm)
```

![Image Alt 텍스트](/assets/images/dacon1_7.PNG){: width="40%" height="40%"}

target은 20을 중심으로 치우쳐 정규성을 띄지 않는다.

```python
stats.probplot(train['Gr Liv Area'], plot=plt)
```

![Image Alt 텍스트](/assets/images/dacon1_8.PNG){: width="40%" height="40%"}

qq 플롯을 그려본 결과, 역시나 정규성을 띄지 않는다. 따라서 로그 변환이 필요하다고 생각된다.

```python
# 먼저 상관계수 계산을 위해 텍스트 형식의 데이터를 숫자로 변환해줍니다.
from sklearn.preprocessing import LabelEncoder

corr_df = train.copy()
corr_df[corr_df.columns[corr_df.dtypes=='O']] = corr_df[corr_df.columns[corr_df.dtypes=='O']].astype(str).apply(LabelEncoder().fit_transform)

#상관관계 분석도
plt.figure(figsize=(15,10))

heat_table = corr_df.corr()
mask = np.zeros_like(heat_table)
mask[np.triu_indices_from(mask)] = True
heatmap_ax = sns.heatmap(heat_table, annot=True, mask = mask, cmap='coolwarm')
heatmap_ax.set_xticklabels(heatmap_ax.get_xticklabels(), fontsize=15, rotation=45)
heatmap_ax.set_yticklabels(heatmap_ax.get_yticklabels(), fontsize=15)
plt.title('correlation between features', fontsize=40)
plt.show()
```

![Image Alt 텍스트](/assets/images/dacon1_9.PNG){: width="50%" height="50%"}

Overall Qual, Gr Liv Area, Garage cars, 1st Flr Sf, Total Bsmt 순으로 타켓변수와 상관 계수가 높다.



<br>

#### Feature Engineering
---

EDA를 통해 Feature Engineering을 진행했다. 상관관계와 변수 특성을 통해 변수 선택과 파생 변수를 추가했다. 

- 이상치가 많지만 데이터 수가 적어 score를 하락시킴 -> **이상치 제거 X**
- 카테고리 값 -> **target과 품질 분포를 바탕으로 라벨 인코딩**
- **파생 변수 추가**
    - 전체 면적
    - 지하실 면적과 품질
    - 전체 훔질
    - 년도 변수 평균 값
    - 완공, 리모델링, 차고 공사 후 얼마나 지났는지
- 타겟 변수: **로그 변환**

```python
qual_cols = train.dtypes[train.dtypes == np.object].index
def label_encoder(data, qual_cols):
  mapping={'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po':1}
  for col in qual_cols :
    data[col] = data[col].map(mapping)

  return data

label_encoder(train, qual_cols)
label_encoder(test, qual_cols)
```

위 EDA를 근거로 라벨 인코딩 진행했다. (OverQual과 target의 분포)

```python
def new_feature(data):
  data['Total Liv Area'] = data['Gr Liv Area'] + data['Total Bsmt SF'] + data['1st Flr SF'] #전체 면적
  data['Bsmt'] = data['Bsmt Qual'] * data['Total Bsmt SF'] #지하실 면적과 품질
  data['Qual'] = data['Overall Qual'] + data['Exter Qual'] + data['Kitchen Qual'] + data['Bsmt Qual'] #전체 품질
  data['Year Ave'] = (data['Year Built'] + data['Year Remod/Add'] + data['Garage Yr Blt'])/3 #년도 평균
  data['Bulit Now Gap'] = 2022-data['Year Built'] #완공한지 얼마나 지났는지
  data['Remote Now Gap'] = 2022-data['Year Remod/Add'] #리모데링한지 얼마나 지났는지
  data['Garage Now Gap'] = 2022-data['Garage Yr Blt'] #차고 만든지 얼마나 지났는지
  
  return data

new_feature(train)
new_feature(test)
```

새로운 특성 변수를 추가했다. 미국집은 한국과 다르게 square feet를 사용한다. 그리고 차고와 지하실을 제외한 나머지 생활 면적이 평수를 계산할 때 중요하다. 따라서 차고와 지하실을 제외한 전체 면적을 위한 파생 변수를 생성했다. 

그리고 미국 집값은 한국과 다르게 방의 개수, 지하실의 면적과 높이가 큰 영향을 미친다. 따라서 자히실의 면적과 품질을 함께 반영할 수 있는 파생 변수를 생성했다.


```python
train.corr().loc["target"].abs().sort_values(ascending=False)
```

![Image Alt 텍스트](/assets/images/dacon1_10.PNG){: width="30%" height="30%"}

새로 생성한 파생 변수가 타겟 변수와의 상관관계가 꽤 높은 것을 확인할 수 있다. 특히 Qual, Total Live Area, Bsmt는 0.8 이상으로 매우 높은 편에 속한다.



<br>

#### Modeling
---

ETR+RF+GBR+XGB+LGB+HGB+NGB+CAT 모델을 사용했고 10FOLD로 앙상블했습니다.
모델별로 10fold의 target 예측값을 만들어 평균 값을 취해주었습니다.

```python
X = train.drop(['id', 'target'], axis=1)
y = np.log1p(train.target)
```

```python
#데이콘 평가 지표
def NMAE(true, pred) -> float:
    mae = np.mean(np.abs(true - pred))
    score = mae / np.mean(np.abs(true))
    return score
```

```python
nmae_score = make_scorer(NMAE, greater_is_better=False)
```

```python
kf = KFold(n_splits = 10, random_state = 42, shuffle = True)
```

- **ExtraTreesRegression**
```python
etr_pred = np.zeros(target.shape[0])
etr_val = []
for n, (tr_idx, val_idx) in enumerate(kf.split(X, y)) :
    tr_x, tr_y = X.iloc[tr_idx], y.iloc[tr_idx]
    val_x, val_y = X.iloc[val_idx], np.expm1(y.iloc[val_idx])
    
    etr = ExtraTreesRegressor(n_estimators=200)
    etr.fit(tr_x, tr_y)
    
    val_pred = np.expm1(etr.predict(val_x))
    val_nmae = NMAE(val_y, val_pred)
    etr_val.append(val_nmae)
    print(f'{n + 1} FOLD NMAE = {val_nmae}')
    
    target_data = Pool(data = target, label = None)
    fold_pred = etr.predict(target) / 10
    etr_pred += fold_pred
print(f'10FOLD Mean of NMAE = {np.mean(etr_val)}')
```

- **RandomForest**
```python
rf_pred = np.zeros(target.shape[0])
rf_val = []
for n, (tr_idx, val_idx) in enumerate(kf.split(X, y)) :
    print(f'{n + 1} FOLD Training.....')
    tr_x, tr_y = X.iloc[tr_idx], y.iloc[tr_idx]
    val_x, val_y = X.iloc[val_idx], np.expm1(y.iloc[val_idx])
    
    rf = RandomForestRegressor(random_state = 42, criterion = 'mae')
    rf.fit(tr_x, tr_y)
    
    val_pred = np.expm1(rf.predict(val_x))
    val_nmae = NMAE(val_y, val_pred)
    rf_val.append(val_nmae)
    print(f'{n + 1} FOLD NMAE = {val_nmae}\n')
    
    fold_pred = rf.predict(target) / 10
    rf_pred += fold_pred
print(f'10FOLD Mean of NMAE = {np.mean(rf_val)} & std = {np.std(rf_val)}')
```

- **GradientBoosting**
```python
gbr_pred = np.zeros(target.shape[0])
gbr_val = []
for n, (tr_idx, val_idx) in enumerate(kf.split(X, y)) :
    print(f'{n + 1} FOLD Training.....')
    tr_x, tr_y = X.iloc[tr_idx], y.iloc[tr_idx]
    val_x, val_y = X.iloc[val_idx], np.expm1(y.iloc[val_idx])
    
    gbr = GradientBoostingRegressor(random_state = 42, max_depth = 4, learning_rate = 0.05, n_estimators = 1000)
    gbr.fit(tr_x, tr_y)
    
    val_pred = np.expm1(gbr.predict(val_x))
    val_nmae = NMAE(val_y, val_pred)
    gbr_val.append(val_nmae)
    print(f'{n + 1} FOLD NMAE = {val_nmae}\n')
    
    fold_pred = gbr.predict(target) / 10
    gbr_pred += fold_pred
print(f'10FOLD Mean of NMAE = {np.mean(gbr_val)} & std = {np.std(gbr_val)}')
```

- **xgbr**

```python
xgb_pred = np.zeros(target.shape[0])
xgb_val = []
for n, (tr_idx, val_idx) in enumerate(kf.split(X, y)) :
    tr_x, tr_y = X.iloc[tr_idx], y.iloc[tr_idx]
    val_x, val_y = X.iloc[val_idx], np.expm1(y.iloc[val_idx])
    
    xgb = XGBRegressor(random_state = 42, learning_rate=0.05, n_estimators=200)
    xgb.fit(tr_x, tr_y)
    
    val_pred = np.expm1(xgb.predict(val_x))
    val_nmae = NMAE(val_y, val_pred)
    xgb_val.append(val_nmae)
    print(f'{n + 1} FOLD NMAE = {val_nmae}')
    
    fold_pred = xgb.predict(target) / 10
    xgb_pred += fold_pred
print(f'10FOLD Mean of NMAE = {np.mean(xgb_val)}')
```

- **lgbr**
```python
lgb_pred = np.zeros(target.shape[0])
lgb_val = []
for n, (tr_idx, val_idx) in enumerate(kf.split(X, y)) :
    tr_x, tr_y = X.iloc[tr_idx], y.iloc[tr_idx]
    val_x, val_y = X.iloc[val_idx], np.expm1(y.iloc[val_idx])
    
    lgb = LGBMRegressor(random_state = 42, learning_rate=0.05, n_estimators=200)
    lgb.fit(tr_x, tr_y)
    
    val_pred = np.expm1(lgb.predict(val_x))
    val_nmae = NMAE(val_y, val_pred)
    lgb_val.append(val_nmae)
    print(f'{n + 1} FOLD NMAE = {val_nmae}')
    
    fold_pred = lgb.predict(target) / 10
    lgb_pred += fold_pred
print(f'10FOLD Mean of NMAE = {np.mean(lgb_val)}')
```

- **HGBRegressor**
```python
hgbr_pred = np.zeros(target.shape[0])
hgbr_val = []
for n, (tr_idx, val_idx) in enumerate(kf.split(X, y)) :
    tr_x, tr_y = X.iloc[tr_idx], y.iloc[tr_idx]
    val_x, val_y = X.iloc[val_idx], np.expm1(y.iloc[val_idx])
    
    hgbr = HistGradientBoostingRegressor(random_state=42, scoring=nmae_score, learning_rate=0.09)
    hgbr.fit(tr_x, tr_y)
    
    val_pred = np.expm1(hgbr.predict(val_x))
    val_nmae = NMAE(val_y, val_pred)
    hgbr_val.append(val_nmae)
    print(f'{n + 1} FOLD NMAE = {val_nmae}')
    
    fold_pred = hgbr.predict(target) / 10
    hgbr_pred += fold_pred
print(f'10FOLD Mean of NMAE = {np.mean(hgbr_val)}')
```


- **CatBoost**
```python
cb_pred = np.zeros(target.shape[0])
cb_val = []
for n, (tr_idx, val_idx) in enumerate(kf.split(X, y)) :
    print(f'{n + 1} FOLD Training.....')
    tr_x, tr_y = X.iloc[tr_idx], y.iloc[tr_idx]
    val_x, val_y = X.iloc[val_idx], np.expm1(y.iloc[val_idx])
    
    tr_data = Pool(data = tr_x, label = tr_y)
    val_data = Pool(data = val_x, label = val_y)
    
    cb = CatBoostRegressor(depth = 4, random_state = 42, loss_function = 'MAE', n_estimators = 3000, learning_rate = 0.03, verbose = 0)
    cb.fit(tr_data, eval_set = val_data, early_stopping_rounds = 750, verbose = 1000)
    
    val_pred = np.expm1(cb.predict(val_x))
    val_nmae = NMAE(val_y, val_pred)
    cb_val.append(val_nmae)
    print(f'{n + 1} FOLD NMAE = {val_nmae}\n')
    
    target_data = Pool(data = target, label = None)
    fold_pred = cb.predict(target) / 10
    cb_pred += fold_pred
print(f'10FOLD Mean of NMAE = {np.mean(cb_val)} & std = {np.std(cb_val)}')
```

- **NGBoost**

```python
from ngboost import NGBRegressor

ngb_pred = np.zeros(target.shape[0])
ngb_val = []
for n, (tr_idx, val_idx) in enumerate(kf.split(X, y)) :
    print(f'{n + 1} FOLD Training.....')
    tr_x, tr_y = X.iloc[tr_idx], y.iloc[tr_idx]
    val_x, val_y = X.iloc[val_idx], np.expm1(y.iloc[val_idx])
    
    ngb = NGBRegressor(random_state = 42, n_estimators = 1000, verbose = 0, learning_rate = 0.03)
    ngb.fit(tr_x, tr_y, val_x, val_y, early_stopping_rounds = 300)
    
    val_pred = np.expm1(ngb.predict(val_x))
    val_nmae = NMAE(val_y, val_pred)
    ngb_val.append(val_nmae)
    print(f'{n + 1} FOLD NMAE = {val_nmae}\n')
    
    target_data = Pool(data = target, label = None)
    fold_pred = ngb.predict(target) / 10
    ngb_pred += fold_pred
print(f'10FOLD Mean of NMAE = {np.mean(ngb_val)} & std = {np.std(ngb_val)}')
```

#### Prediction & Submission
---


``` python
etr_pred + rf_pred + gbr_pred + xgb_pre + hgbr_pred + ngb_pred + cat_pred / 7
```

각 모델이 10 fold마다 만든 예측값의 산술 평균이다. 이 값을 지수 변환을 통해 원래 값으로 복원하여 target에 저장했다.

```python
submission['target'] = (np.expm1(etr_pred) + np.expm1(rf_pred) + np.expm1(gbr_pred) + np.expm1(xgb_pred) + np.expm1(hgbr_pred) + 
            np.expm1(ngb_pred) + np.expm1(ngb_pred)) / 7

submissionsubmission.to_csv('1st.csv', index = False)
```


---

<p style="font-size: 1.0em">[References] 모델링을 진행할 때 아래 두 코드 공유를 참고해 진행했다.  </p>

<a href = "https://dacon.io/competitions/official/235869/codeshare/4266?page=1&dtype=recent
" style="font-size: 0.8em"> GB + RF + CB + NGB / Public : 0.09599 </a>      
<a href = "https://dacon.io/competitions/official/235869/codeshare/4450?page=1&dtype=recent" style="font-size: 0.8em"> Private 1위 (0.09847) ETR+RF+GBR+XGB+HGB+NGB+CAT</a>       

<p style="font-size: 1.0em"> [느낀 점] </p>
- 다양한 모델을 사용하여 fold별 val 평균값으로 예측할 수 있다는 것을 처음 알게 되었다.
- 미국 집값에 대한 주요한 원인이 지하실 차고와 전체 면적이다. (파생 변수의 중요성)
- 데이터 분석을 진행할 때 그 분야의 배경 지식을 아는 것이 중요하다는 것을 알게 되었다. 