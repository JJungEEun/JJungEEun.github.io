---
layout: post
title: "[GAN] pix2pix"
date: 2022-04-05 08:44:38 -0400
category: deep-learning
author: eun
short-description: 생성적 적대 신경망 아키텍처를 사용한 화풍모사, Pix2Pix
toc: true
use_math: true

---

#### pix2pix
---

<p align="center"><img src="/assets/images/pix2pix.png"  width="60%" height="60%"></p>

Pix2Pix는 이미지를 이미지로 변환하도록 genertor를 학습한다. 예를 들어, generator의 입력값으로 스케치 그림을 입력하면 완성된 그림이 나오도록 학습할 수 있다. 기존 GAN과 비교하여 설명하자면, **pix2pix**는 기존 GAN의 noise 대신 스케치 그림을 입력하여 학습하는 것이다. 

- generator: 스케치를 입력받아 가짜 이미지를 출력
- discriminator: 생성기가 생성한 가짜 이미지를 완성된 그림으로 식별하도록 목적 함수를 설계하여 학습을 진행한다. 


<p style="font-size: 1.12em">📌pix2pix 목표</p>
---


<!-- $G, D$를 각각 생성 모델과 판별 모델이라 하자. $x, y, z$를 각각 입력 이미지, 출력 이미지, 노이즈라고 하자. -->

<!-- cGAN의 목표는 다음과 같이 정의 되는 c -->


<p style="font-size: 1.00em"><b>Generator</b></p>

<p align="center"><img src="/assets/images/pix2pix_03.PNG"  width="45%" height="45%"></p>

Generator은 왼쪽과 같은 Input을 받아 오른쪽과 같은 출력값을 생성하며 Discriminator을 속이는 것을 목적으로 한다. 

<p align="center"><img src="/assets/images/pix2pix_01.PNG"  width="60%" height="60%"></p>


L1 손실 함수를 사용하면 생성기가 blur가 적용된 row-frequency(저해상도) 이미지를 생성한다. 위 그림 속 L1만 사용한 그림을 보면 매우 흐릿한 것을 확인 할 수 있다. 기존의 gan 손실 함수에 L1 손실 함수를 추가하면 high-frequency(고해상도) 이미지를 생성한다. 



<p align="center"><img src="/assets/images/pix2pix_04.PNG" width="45%" height="45%"></p>

Pix2Pix는 이미지 생성 성능을 높이기 위해 generator 모델 구조로 U-NET을 사용한다. 

<!-- ```python
class Generator(object):
    def __init__(self, width = 28, height= 28, channels = 1):
        
        self.W = width
        self.H = height
        self.C = channels
        self.SHAPE = (width,height,channels)

        self.Generator = self.model()
        self.OPTIMIZER = Adam(lr=2e-4, beta_1=0.5,decay=1e-5)
        self.Generator.compile(loss='binary_crossentropy', optimizer=self.OPTIMIZER,metrics=['accuracy'])

        self.summary()


    # 가짜 이미지를 생성
    def model(self):
        input_layer = Input(shape=self.SHAPE)
        
        down_1 = Convolution2D(64  , kernel_size=4, strides=2, padding='same',activation=LeakyReLU(alpha=0.2))(input_layer)
        norm_1 = InstanceNormalization()(down_1)

        down_2 = Convolution2D(64*2, kernel_size=4, strides=2, padding='same',activation=LeakyReLU(alpha=0.2))(norm_1)
        norm_2 = InstanceNormalization()(down_2)

        down_3 = Convolution2D(64*4, kernel_size=4, strides=2, padding='same',activation=LeakyReLU(alpha=0.2))(norm_2)
        norm_3 = InstanceNormalization()(down_3)

        down_4 = Convolution2D(64*8, kernel_size=4, strides=2, padding='same',activation=LeakyReLU(alpha=0.2))(norm_3)
        norm_4 = InstanceNormalization()(down_4)


        upsample_1 = UpSampling2D()(norm_4)
        up_conv_1 = Convolution2D(64*4, kernel_size=4, strides=1, padding='same',activation='relu')(upsample_1)
        norm_up_1 = InstanceNormalization()(up_conv_1)
        add_skip_1 = Concatenate()([norm_up_1,norm_3])

        upsample_2 = UpSampling2D()(add_skip_1)
        up_conv_2 = Convolution2D(64*2, kernel_size=4, strides=1, padding='same',activation='relu')(upsample_2)
        norm_up_2 = InstanceNormalization()(up_conv_2)
        add_skip_2 = Concatenate()([norm_up_2,norm_2])

        upsample_3 = UpSampling2D()(add_skip_2)
        up_conv_3 = Convolution2D(64, kernel_size=4, strides=1, padding='same',activation='relu')(upsample_3)
        norm_up_3 = InstanceNormalization()(up_conv_3)
        add_skip_3 = Concatenate()([norm_up_3,norm_1])

        last_upsample = UpSampling2D()(add_skip_3)
        output_layer = Convolution2D(3, kernel_size=4, strides=1, padding='same',activation='tanh')(last_upsample)
        
        return Model(input_layer,output_layer)
```
-->

<p style="font-size: 1.00em"><b>Discriminator</b></p>

pix2pix는 판별기를 patch gan을 사용한다. 

- patch gan: 출력값으로 하나의 scalar 값을 출력하는 것이 아니라 이미지를 분할한 피처맵을 출력한다. 
    + 256X256 크기의 이미지를 입력 받은 경우, 30X30의 출력값을 생성함
    + 원래 이미지가 30X30 feature map으로 분할하여 각 pixel을 real, fake인지 식별하는 것이다.
    + 조건부 gan이기 때문에, 조건부 데이터를 입력받는다. 아래 그림 두개를 한꺼번에 입력받음
    + <p><img src="/assets/images/pix2pix_02.png"  width="30%" height="30%"></p> 

<!-- ```python
class Discriminator(object):
    def __init__(self, width = 28, height= 28, channels = 1):
        self.W = width
        self.H = height
        self.C = channels
        self.CAPACITY = width*height*channels
        self.SHAPE = (width,height,channels)
        
        self.Discriminator = self.model()
        self.OPTIMIZER = Adam(lr=2e-4, beta_1=0.5,decay=1e-5)
        self.Discriminator.compile(loss='mse', optimizer=self.OPTIMIZER, metrics=['accuracy'] )

        self.summary()

    # 가짜 이미지 판별: patch gan을 사용
    # patch gan: 이미지를 
    def model(self):
        input_layer = Input(self.SHAPE)

        up_layer_1 = Convolution2D(64, kernel_size=4, strides=2, padding='same',activation=LeakyReLU(alpha=0.2))(input_layer)

        up_layer_2 = Convolution2D(64*2, kernel_size=4, strides=2, padding='same',activation=LeakyReLU(alpha=0.2))(up_layer_1)
        norm_layer_1 = InstanceNormalization()(up_layer_2)

        up_layer_3 = Convolution2D(64*4, kernel_size=4, strides=2, padding='same',activation=LeakyReLU(alpha=0.2))(norm_layer_1)
        norm_layer_2 = InstanceNormalization()(up_layer_3)

        up_layer_4 = Convolution2D(64*8, kernel_size=4, strides=2, padding='same',activation=LeakyReLU(alpha=0.2))(norm_layer_2)
        norm_layer_3 =InstanceNormalization()(up_layer_4)

        output_layer = Convolution2D(1, kernel_size=4, strides=1, padding='same')(norm_layer_3)
        output_layer_1 = Flatten()(output_layer)
        output_layer_2 = Dense(1, activation='sigmoid')(output_layer_1)
        
        return Model(input_layer,output_layer_2)
``` -->


patch gan을 사용하면 high-frequency의 정확도가 높아진다
+ high-frequency: 고해상도
+ high-frequency의 정확도가 높아진다: 디테일한 부분이 향상된다.

<p align="center"><img src="/assets/images/pix2pix_05.PNG" width="60%" height="60%"></p>

분할된 patch를 기준으로 진짜인지 가짜인지 식별하기 때문데 더 디테일한 결과를 출력할 수 있다.
위 그림처럼 이미지를 많이 분할할 수록 high-frequency가 높아진다.


---
<p style="font-size: 1.0em">References</p>

<a href = "https://github.com/wikibook/gan" style="font-size: 0.8em"> 실전 예제로 배우는 GAN(위키북스) </a>      
<a href = "https://arxiv.org/pdf/1611.07004.pdf" style="font-size: 0.8em"> Image-to-Image Translation with Conditional Adversarial Networks </a>      
